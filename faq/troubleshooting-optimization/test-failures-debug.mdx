---
title: "Test Failures and Diagnostics"
description: "Advice for interpreting test failures, debugging test results, leveraging output and logging features, and avoiding common mistakes that can mislead or block test validation."
---

# Test Failures and Diagnostics

This documentation page guides you through interpreting test failures, debugging test results, using output and logging features effectively, and avoiding common mistakes that can block or mislead test validation in GoogleTest and GoogleMock.

---

## 1. Understanding Test Failures

### Types of Failures
- **Fatal Failures (`ASSERT_*` macros)**: Abort the current function immediately, preventing further execution within the same test.
- **Nonfatal Failures (`EXPECT_*` macros)**: Record failure but allow the test function to continue running.

### How Failures Affect Tests
- A test is marked **failed** if it contains any fatal or nonfatal failure.
- A test **passes** if no failures occur and it does not crash.

### Common Failure Scenarios
- **Expected call failures**: When a mock's expected method call is not made or called with incorrect arguments.
- **Unexpected or uninteresting calls**: When a mock method is called unexpectedly or without expectations.
- **Excessive calls**: More calls than specified by expectation cardinality.
- **Unsatisfied prerequisites** in sequences or call order.
- **Argument or matcher mismatches**: Call arguments do not satisfy expected matchers or `.With()` conditions.

<Tip>
Understanding the difference between fatal and nonfatal failures helps you choose assertion macros to prevent cascading errors and facilitate debugging.
</Tip>

---

## 2. Interpreting GoogleMock Output Messages

When a test fails due to mock expectations, GoogleMock provides detailed messages:

### Common Messages & What They Mean
- **Expected call**: An anticipated method call occurred correctly.
- **Unexpected call**: A method was called unexpectedly (no matching expectation).
- **Excessive call**: Method was called more times than allowed.
- **Uninteresting call**: Call happened but had no matching expectation and no warnings (if NaggyMock or NiceMock in use).
- **Retired expectation**: An expectation has ended but the call matches a previous retired one.
- **Mismatch arguments**: The actual call's arguments differ from those expected; the output will show the actual vs expected values.
- **Unsatisfied prerequisite**: The expected call order or sequence is not maintained.

For example, a mismatch arguments failure shows:

```
Value of: foo_.Bar(Ref(s), _, Ge(0))
  Actual: "Ho", 0, -0.1
Expected: "Hi", 0, >= 0
```

This lets you quickly identify which argument caused the failure.

---

## 3. Using Verbose Output and Logging

GoogleMock and GoogleTest support verbosity control to provide more detailed output when needed.

### Setting Verbosity
- Use the `--gmock_verbose` flag or programmatically call `GMOCK_FLAG_SET(verbose, "level")`.
- Levels include `info`, `warning`, `error`, and `warning` (default).

### Benefits
- **Info** verbosity shows all expected calls as they happen.
- Helps track method call order and which expectations fired.

### Logging Additional Information
- Use `RecordProperty(key, value)` in tests to log extra data, visible in XML output reports.

Example:
```cpp
RecordProperty("DatabaseState", db.GetState());
```

<Tip>
Enabling verbose output is especially useful to trace why some expected calls were not made or why unexpected calls occur.
</Tip>

---

## 4. Best Practices for Debugging Test Failures

### Use Scoped Traces
- `SCOPED_TRACE("message")` adds context to failures from nested helper functions.
- This helps identify the calling context of failures especially when assertions occur in subroutines.

Example:
```cpp
SCOPED_TRACE("In ProcessingStep 3");
EXPECT_EQ(result, expected);
```

### Check Call Order and Sequences
- Use `Sequence` and `InSequence` to control and verify call ordering.
- Failures about unsatisfied prerequisites indicate order problems.

### Validate Mock Expectations
- Confirm `EXPECT_CALL` declarations match actual calls including arguments.
- Use wildcards (`_`) when specific argument values do not matter.
- Adjust cardinalities (`Times`, `AtLeast`, `AtMost`) appropriately.

### Handle Uninteresting Calls
- Use `NaggyMock` (default) to warn on uninteresting calls.
- Use `NiceMock` to suppress such warnings.
- Use `StrictMock` to treat any uninteresting call as a failure.

### Retire Expectations When Needed
- Use `.RetiresOnSaturation()` to disable an expectation after it's fully satisfied, avoiding excessive call failures.

---

## 5. Common Pitfalls and How to Avoid Them

| Pitfall                               | Description                                       | Solution                                         |
| ------------------------------------ | ------------------------------------------------ | ------------------------------------------------ |
| Forgetting `EXPECT_CALL` before use  | Leads to unexpected call failures                | Always set expectations before exercising mocks |
| Overly strict argument matching      | Causes false negatives when exact values aren't needed | Use wildcards (`_`) or flexible matchers (`Ge()`, `Lt()`)  |
| Wrong call order assumption          | Unsatisfied prerequisite failures occur          | Use sequences or relaxed order constraints       |
| Relying on mock behavior without setting it | Unexpected behavior or default return values     | Specify return behaviors via `WillOnce()`, `WillRepeatedly()`, or `ON_CALL()` |
| Not resetting mock state between tests | Tests become flaky or produce spurious failures  | Create fresh mock objects per test or tear down shared mocks |

<Tip>
To prevent failures due to unhandled arguments or ordering, start with broad matchers and relax as needed.
</Tip>

---

## 6. Step-by-Step Troubleshooting Workflow

<Steps>
<Step title="Identify Failure Type and Location">
Review failure output carefully. Look for hints like argument mismatch, unexpected call, or unsatisfied prerequisites.
</Step>
<Step title="Enable Verbosity">
Run the test with `--gmock_verbose=info` to get detailed call traces.
</Step>
<Step title="Review Mock Expectations">
Check the `EXPECT_CALL` and `ON_CALL` declarations match reality.
</Step>
<Step title="Add Scoped Traces">
Annotate helper functions with `SCOPED_TRACE` to isolate failure origin.
</Step>
<Step title="Check Call Sequences and Ordering">
Verify if sequences or `InSequence` are correctly set and adhered to.
</Step>
<Step title="Adjust Mock Behavior and Cardinalities">
Relax argument matchers or expand cardinalities as necessary.
</Step>
<Step title="Run Isolated Tests">
Run the failing test alone to eliminate interference.
</Step>
<Step title="Confirm Fix and Regenerate Golden Output">
Ensure failures are resolved and output messages make sense.
</Step>
</Steps>

---

## 7. Understanding Common Error Messages

### Explicit Actions Run Out
If `EXPECT_CALL` with `WillOnce` is called more than specified times, subsequent calls produce this failure.

### Unsatisfied Expectation
Method expected with certain arguments was never called or called fewer times than expected.

### Excessive Calls
Method called more times than allowed by the times clause.

### Mismatch Arguments
Actual arguments passed do not satisfy the expected matchers.

### Unexpected Call
Call was made to a mock method that has no matching expectation.

---

## 8. Using Diagnostic Tools and Assertions

### ASSERT_NO_FATAL_FAILURE and EXPECT_NO_FATAL_FAILURE
Use these in tests to verify that a subroutine passes without fatal failures.

### HasFatalFailure and HasNonfatalFailure
Check whether the current test has any fatal or nonfatal failures programmatically.

### Catching Failures
Use
```cpp
EXPECT_FATAL_FAILURE(statement, "message substring");
EXPECT_NONFATAL_FAILURE(statement, "message substring");
```
to assert that certain code triggers failures, helping test your testing utilities.

---

## 9. Reporting and Analyzing Test Results

### XML and JSON Output
- Use the `--gtest_output=xml:file.xml` or `--gtest_output=json:file.json` flags to produce rich test reports.
- Include detailed properties with `RecordProperty()` for integration with CI.

### Break on Failure
Set `--gtest_break_on_failure` to stop execution in debugger on failure.

### Filtering and Running Tests
Use `--gtest_filter=...` to run subsets of tests to isolate issues.

---

## 10. Summary and Further Reading

This page empowers you to interpret failures confidently, debug effectively, and avoid common mistakes in GoogleTest and GoogleMock.

For comprehensive understanding, consult:

- [GoogleTest Primer](docs/primer.md) for basic test writing and assertions.
- [GoogleMock Cookbook](gmock_cook_book.md) for deep mocking guidance.
- [Assertions Reference](reference/assertions.md) for full assertion API.
- [Mocking Reference](reference/mocking.md) for mock-related APIs and best practices.
- [Troubleshooting Common Setup Issues](getting-started/first-run-validation/troubleshooting-common-issues.mdx) for setup-related problems.

Use these resources to deepen your mastery and improve test reliability.

---


---

### Code Example: Diagnosing Argument Mismatch

```cpp
using ::testing::Ref;
using ::testing::Ge;
using ::testing::_;

class MockFoo {
public:
  MOCK_METHOD3(Bar, char(const std::string& s, int i, double x));
};

TEST(MockFooTest, BarMismatch) {
  MockFoo foo;
  const std::string expectedStr = "Hi";

  EXPECT_CALL(foo, Bar(Ref(expectedStr), _, Ge(0)));

  // This call will cause failure: arguments mismatch
  foo.Bar("Ho", 0, -0.1);

  // This call matches expectations
  foo.Bar(expectedStr, 0, 0);
}
```

The test output will highlight which argument(s) caused the mismatch with actual and expected values.

---

### Tips
- Always set expectations before exercising mock methods.
- Use `SCOPED_TRACE` to add context for easier debugging.
- Enable verbose output to get detailed call logs.
- Favor `EXPECT_*` for nonfatal checks during iterative debugging.
- Use `.RetiresOnSaturation()` to handle overlapping expectations elegantly.

---

### Troubleshooting Checklist

- [ ] Are all `EXPECT_CALL`s issued before using mocks?
- [ ] Are argument matchers flexible enough for your test context?
- [ ] Did you set appropriate call cardinality?
- [ ] Are you observing call order requirements or relaxed accordingly?
- [ ] Is verbose output enabled to trace failure origins?
- [ ] Are mocks properly instantiated per test to avoid stale state?
- [ ] Use `SCOPED_TRACE` in inner functions to clarify failures.

---

This structured approach will help you navigate test failures confidently, reducing debugging time and improving test suite stability.
