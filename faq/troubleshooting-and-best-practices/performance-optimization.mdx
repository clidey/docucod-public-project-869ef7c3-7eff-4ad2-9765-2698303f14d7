---
title: "How can I optimize the performance of my tests?"
description: "Techniques and tips to make large or slow test suites faster, including leveraging parallel test execution, controlling test discovery, and best practices for writing efficient mocks and assertions."
---

# How can I optimize the performance of my tests?

Optimizing the performance of your GoogleTest and GoogleMock suites can dramatically reduce build and test times, improve developer productivity, and make your continuous integration pipelines more efficient. This page provides practical techniques and best practices specifically for speeding up large or slow test suites by leveraging parallel execution, controlling test discovery, and writing efficient mocks and assertions.

---

## 1. Leverage Parallel Test Execution

### Why Parallelize?
Large test suites can take a long time to run when tests are executed serially. Since GoogleTest supports running multiple tests concurrently, harnessing parallelism allows you to reduce overall test time proportionally to the number of CPU cores available.

### How to Enable Parallel Tests
- Use **`--gtest_parallel`** or external parallel test runners if available in your build system.
- Split tests into multiple executables targeting isolated test domains.
- Utilize build system features like Bazel's test shards or CTest parallel tests.

### Best Practices
- Ensure your tests are **independent and stateless** to avoid flaky parallel runs.
- Avoid shared mutable global state or synchronize access properly.
- Parameterize tests if a test needs to run with multiple configurations.

<Tip>
Parallelizing tests can be the single biggest win in reducing test suite runtime, especially when your project has many CPU cores available.
</Tip>

---

## 2. Control Test Discovery and Filtering

### Test Discovery Overhead
GoogleTest discovers all tests by scanning compiled binaries, which can take longer if your suite has thousands of tests.

### Filtering Tests
- Use the `--gtest_filter=` flag to selectively run only a subset of tests relevant to your changes.
- Combine with build system dependencies to determine minimal test sets.

### Disabling Unneeded Tests
- Temporarily disable rarely used or long-running tests during development by using the `DISABLED_` prefix.
- Organize tests meaningfully to easily identify slow or flaky ones.

### Organizing Large Suites
For large projects, see [Organizing Large Test Suites](/guides/writing-tests/organizing-test-suites) for patterns on grouping, naming, and fixture reuse, which helps manage discovery time and test execution efficiency.

<Note>
Using test filters not only accelerates runs but also helps focus on current areas under development.
</Note>

---

## 3. Write Efficient Mocks and Assertions

### Use ON_CALL Wisely
- Use `ON_CALL()` to set default behaviors for mock methods without imposing call count constraints.
- Avoid excessive use of `EXPECT_CALL()` for methods where invocation is not your primary concern.
- Prefer `ON_CALL()` for general default behavior and only set `EXPECT_CALL()` where you need to verify interactions.

### Control Mock Strictness
- Use `NiceMock<T>` to suppress warnings for uninteresting calls and reduce noise in the output.
- Use `StrictMock<T>` only when you want to enforce that no uninteresting calls occur, as this can slow down tests.

### Minimize Complex Matchers and Actions
- Avoid overly complex matchers that may incur performance penalties.
- Cache frequently reused matchers in variables rather than reconstructing them repeatedly.
- Prefer simple built-in matchers whenever possible.

### Reuse Actions and Return Values
- For actions that can be shared, define them once and reuse to avoid overhead.
- For move-only types (e.g., `std::unique_ptr`), use lambdas to create fresh objects on each call, rather than returning stored objects.

<Tip>
Remember that mock expectations are searched in reverse order of declaration; place general expectations first and more specific ones later to avoid runtime mismatches.
</Tip>

---

## 4. Reduce Test Setup and Teardown Overhead

- Share fixtures using test suites and `TEST_F` to reduce repeated initialization code.
- Avoid expensive global or static initialization. Use lazy initialization or setup once per test suite.
- Use `SetUpTestSuite()` and `TearDownTestSuite()` for shared setup for all tests in a suite.

---

## 5. Opt for Lightweight Assertions

- Use simple assertions (`EXPECT_EQ`, `EXPECT_TRUE`) when possible.
- For complex data structures, prefer expressive matchers only where necessary to aid diagnosis.
- Avoid using heavyweight assertions repeatedly in tight loops.

---

## 6. Monitor and Profile Test Performance

- Employ your build system or CI's test timing reports.
- Use GoogleTest's own timing flags (e.g., `--gtest_print_time`) to identify slow tests.
- Profile tests to detect bottlenecks in mocks, IO, or computation.

---

## Troubleshooting Common Performance Issues

<AccordionGroup title="Common Test Performance Pitfalls">
<Accordion title="Tests Not Running in Parallel">
Check that your test framework command or build tool actually uses parallel execution. Also, confirm tests do not share mutable state causing deadlocks or race conditions.
</Accordion>
<Accordion title="Slow Mock Expectations">
Avoid complex or excessive expectations. Consolidate matchers and order constraints only where necessary.
</Accordion>
<Accordion title="Expensive Test Setup or Teardown">
Review fixture setup. Use suite-level setup if applicable.
</Accordion>
</AccordionGroup>

---

## Additional Resources & Next Steps
- [Organizing Large Test Suites](/guides/writing-tests/organizing-test-suites)
- [Building & Using Mocks](/guides/mocking-advanced-usage/building-mocks)
- [Optimizing Test Performance](/guides/real-world-patterns/optimizing-test-performance)
- [Debugging Common Failures](/guides/real-world-patterns/debugging-common-failures)


---

## Example: Using `ON_CALL` and `EXPECT_CALL` for Performance-Sensitive Mocks

```cpp
using ::testing::Return;
using ::testing::_;  // wildcard matcher

class MockWidget : public IWidget {
 public:
  MOCK_METHOD(int, GetValue, (), (override));
  MOCK_METHOD(void, SetValue, (int val), (override));
};

TEST(SomeTest, UsesWidget) {
  MockWidget mock;

  // Set default mock behavior for all calls to GetValue
  ON_CALL(mock, GetValue())
      .WillByDefault(Return(42));

  // Expect SetValue to be called specifically with 10, twice
  EXPECT_CALL(mock, SetValue(10))
      .Times(2);

  // Code under test exercising mock
  int v = mock.GetValue(); // returns 42 without explicit EXPECT_CALL
  mock.SetValue(10);
  mock.SetValue(10);
}
```

This example suppresses unnecessary warnings for `GetValue()` while being strict on calls to `SetValue(10)`, helping keep runtime and output lean.

---

## Summary
Optimizing test performance requires a combination of parallel execution, selective test discovery, efficient mock usage, and streamlined setup. By applying these techniques, your tests will run faster and remain maintainable.

<Tip>
Regularly profile and monitor your test suite to catch performance regressions early.
</Tip>
