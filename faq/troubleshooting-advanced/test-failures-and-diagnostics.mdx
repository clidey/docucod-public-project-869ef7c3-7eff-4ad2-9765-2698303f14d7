---
title: "Understanding Test Failures and Diagnostics"
description: "Explains how to interpret test output, diagnose assertion failures, and leverage framework features for debugging failed tests. Offers guidance for using output files, macros, and diagnostic utilities."
---

# Understanding Test Failures and Diagnostics

This page guides you through interpreting test output, diagnosing why tests fail, and using GoogleTest features for effective debugging. You'll learn how to read assertion failure messages, leverage output files and macros, and use diagnostic utilities to pinpoint issues and resolve them efficiently.

---

## 1. Interpreting Test Output

When a test fails, GoogleTest generates detailed messages that help identify the root cause. Understanding these outputs is crucial for fast issue resolution.

### Types of Failures
- **Fatal failures (`ASSERT_*`)**: Abort the current function immediately, preventing further execution in that test. 
- **Nonfatal failures (`EXPECT_*`)**: Log the failure but allow the test to continue running.

### Key Elements in Failure Messages
- **Test location**: The file and line number where the failure occurred.
- **Failure type**: Fatal or nonfatal.
- **Assertion expression**: The condition or comparison that failed.
- **Actual and expected values**: Helps identify mismatches.
- **Stack trace** (if enabled): Shows call stack leading to failure to assist debugging.

Example:

```none
path/to/test_file.cc:42: Failure
Value of: foo()
  Actual: 3
Expected: 5
Stack trace:
  ...
```

This means the call to `foo()` returned `3` when `5` was expected, and the failure happened at line 42.

### Use Custom Failure Messages
Assertion macros support streaming custom messages for additional context:

```cpp
EXPECT_TRUE(condition) << "Additional explanation for failure";
```

---

## 2. Diagnosing Assertion Failures

### Common Assertion Types and Their Meanings
Reference the [Assertions Reference](reference/assertions.md) for details, but key assertions commonly triggering failures include:

- **Equality checks**: `EXPECT_EQ(a, b)` fails when `a != b`.
- **Boolean conditions**: `EXPECT_TRUE(condition)` fails when `condition` is false.
- **String comparisons**: Use `EXPECT_STREQ` for C strings to check content equality.
- **Floating-point comparisons**: Use `EXPECT_FLOAT_EQ` or `EXPECT_NEAR` to handle rounding nuances.
- **Exception assertions**: `EXPECT_THROW` ensures code throws specific exceptions.

### Diagnosing Failure Messages
Failure messages carefully include values involved and often show diffs for strings with embedded nulls or multi-line content. For example, mismatched string assertions display unified diffs highlighting exact differences.

### Tips for Detailed Assertions
- Use *predicate assertions* like `EXPECT_PRED*` to get detailed messages for complex conditions.
- Use *predicate-formatter assertions* (`EXPECT_PRED_FORMAT*`) to fully customize failure messages for complex types.
- Leverage the streaming operator (`<<`) to add explanatory text alongside assertions.

---

## 3. Using Output Files for Diagnostics

GoogleTest can generate detailed XML or JSON output reports summarizing test runs, especially useful in CI environments or for post-run analysis:

- Use the `--gtest_output` flag to specify report format and file/directory, for example:
  - `--gtest_output=xml`: writes XML report to `test_detail.xml`.
  - `--gtest_output=xml:folder/`: places XML report in `folder/`.
  - `--gtest_output=json`: writes a JSON report.

- Reports include:
  - Test suite and test case names.
  - Pass/fail status.
  - Timing information.
  - Failure details and messages.

These files facilitate automated test result processing and integration with dashboards.

---

## 4. Leveraging Macros and Diagnostic Utilities

### Scoped Traces (SCOPED_TRACE and ScopedTrace)

When failures occur deep inside helper functions or loops, `SCOPED_TRACE` helps add contextual information about where and why the failure happened.

Example:

```cpp
void HelperFunction(int x) {
  SCOPED_TRACE("HelperFunction called with x = " << x);
  EXPECT_EQ(x % 2, 0);
}

TEST(MyTest, UsesHelper) {
  for (int i = 0; i < 3; ++i) {
    HelperFunction(i);
  }
}
```

Failure messages will include the trace showing which iteration caused the failure.

### Catching Failures in Subroutines

Fatal assertions (`ASSERT_*`) abort only the current function. To propagate failure to caller functions, use:

- `ASSERT_NO_FATAL_FAILURE(statement)`: verifies `statement` does not cause fatal failures.
- `HasFatalFailure()`: check if any fatal failures have occurred so far.

This ensures that after a failing helper function returns, you can abort the current test early to avoid dangerous continued execution.

Example:

```cpp
void Helper() {
  ASSERT_EQ(1, 2);
}

TEST(MyTest, UsesHelper) {
  Helper();
  if (HasFatalFailure()) return; // Stops here if Helper failed
  // Safe to continue
}
```

### Using ScopedFakeTestPartResultReporter for Testing Failures

When developing testing tools or extensions, `ScopedFakeTestPartResultReporter` can intercept failures programmatically, enabling assertions about expected failures using macros:

- `EXPECT_FATAL_FAILURE(statement, substring)`
- `EXPECT_NONFATAL_FAILURE(statement, substring)`

These macros verify that `statement` generates exactly one failure containing `substring`.

---

## 5. Best Practices and Common Pitfalls

- **Check test filter and disabled tests**: Only the tests matching filters and non-disabled tests run by default. Use `--gtest_also_run_disabled_tests` to include disabled tests.
- **Use clear failure messages**: Stream custom messages for richer test diagnostics.
- **Use appropriate assertion types**: Use `ASSERT_*` when subsequent code depends on a condition being true; otherwise, prefer `EXPECT_*`.
- **Avoid side effects in assertions**: Assertion arguments are evaluated once but avoid relying on side effects.
- **Leverage test suite and global setup/teardown** to speed tests by sharing expensive resources.
- **Enable stack traces** selectively with `--gtest_stack_trace_depth`.
- **Use death tests appropriately** for testing fatal error conditions that terminate the process.

---

## 6. Troubleshooting Common Scenarios

### Failure: Test Continues after Fatal Assertion
- Remember `ASSERT_*` aborts only the current function.
- Use `ASSERT_NO_FATAL_FAILURE()` or `HasFatalFailure()` in caller functions to stop execution.

### Failure: Unexpected Output or Missing Failures in Reports
- Verify command-line flags like `--gtest_output`, `--gtest_filter`, and `--gtest_also_run_disabled_tests`.
- Check if environment variables related to sharding (`GTEST_TOTAL_SHARDS`, `GTEST_SHARD_INDEX`) are appropriately set.

### Failure: Stack Trace Missing or Unhelpful
- Ensure `--gtest_stack_trace_depth` is set higher than 0.
- Use `SCOPED_TRACE` for additional context inside helper functions.

### Failure: Tests Seem Not to Run
- Check for `DISABLED_` prefix in test suite or test names.
- Confirm your filter includes intended test names.
- Validate correct linking and registration of tests.

---

## 7. Next Steps

- Explore [Assertions Reference](reference/assertions.md) to deepen your understanding of assert macros.
- Learn about [Advanced Testing Techniques](guides/advanced-usage-integration/custom-assertions-matchers) for writing custom assertions.
- Use [Value-Parameterized Tests](api-reference/core-testing-apis/parameterized-and-typed-tests) to scale tests efficiently.
- Integrate output reports with CI using XML/JSON output.

---

## Related Documentation
- [Assertions Reference](reference/assertions.md)
- [Testing Reference](docs/reference/testing.md)
- [Advanced GoogleTest Topics](docs/advanced.md)
- [Test Structure and Lifecycle](api-reference/core-testing-apis/test-structure-and-lifecycle)
- [Global Set-Up and Tear-Down](docs/advanced.md#global-set-up-and-tear-down)

---

## Practical Example: Diagnosing a Failed Test

Suppose a test prints:

```none
/path/to/my_test.cc:23: Failure
Value of: add(2, 2)
  Actual: 5
Expected: 4
```

Steps to diagnose:

1. Check the line with the failure (`my_test.cc` line 23).
2. Review the expected vs actual values - here, `add(2, 2)` returned 5 instead of the expected 4.
3. Add further `SCOPED_TRACE` or streaming in `EXPECT_EQ(add(2, 2), 4) << "Hint: Check addition implementation";`.
4. Use the XML report after running with `--gtest_output=xml:test_results.xml` for CI integration or automated analysis.

---

## Troubleshooting Checklist

- Are assertion messages clear enough? Add streaming explanations if needed.
- Are you using the right assertion variants (`ASSERT_*` vs `EXPECT_*`)?
- Are tests filtered unintentionally? Verify filters and disabled tests.
- Are stack traces enabled and informative? Adjust `--gtest_stack_trace_depth`.
- Have you considered test sharding effects if running on multiple machines?

---

We empower you to confidently interpret your test failures and leverage framework capabilities for efficient diagnostics and smoother testing cycles.