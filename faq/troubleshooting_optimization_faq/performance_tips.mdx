---
title: "How can I optimize test execution speed and throughput?"
description: "Provides concrete tips for reducing test suite runtime, such as test selection, parallel execution, and avoiding common slowdown patterns. Directs users to best practices for fast, reliable continuous integration."
---

# How can I optimize test execution speed and throughput?

Optimizing your test suite's execution speed and throughput is essential for maintaining an efficient developer workflow and quick feedback cycles in continuous integration (CI). This page offers actionable tips and best practices that help you reduce runtime without sacrificing test reliability or coverage.

---

## Key Strategies to Speed Up Test Execution

### 1. Selective Test Execution

Focus on running only the tests relevant to recent changes or specific contexts to save time.

- **Use test filtering:** Leverage GoogleTest's `--gtest_filter` flag to run subsets of tests by their suite or test names. For example:
  ```sh
  ./my_tests --gtest_filter=MySuite.*
  ```
  runs all tests in `MySuite` only.
- **Run disabled tests selectively:** Normally, disabled tests marked with `DISABLED_` prefix are skipped. You can include them using `--gtest_also_run_disabled_tests` when necessary.
- **Test sharding:** Distribute your test suite across multiple machines or processes via sharding using `GTEST_TOTAL_SHARDS` and `GTEST_SHARD_INDEX` environment variables, so each shard runs a fraction of the tests concurrently.

### 2. Parallel Test Execution

GoogleTest supports parallel execution at the test program level through CMake, Bazel, or your CI infrastructure.

- **Run multiple test binaries concurrently:** Partition tests into multiple test executables and run them in parallel.
- **Leverage CI parallelism:** Configure your CI pipelines to run shards or test executables in parallel containers or agents.

### 3. Avoiding Test Suite Slowdowns

Some patterns can unintentionally slow down your tests. Avoid these to maintain speed:

- **Minimize global or suite-wide setup costs:** Shared expensive resources should be initialized efficiently using `SetUpTestSuite()` and cleaned up with `TearDownTestSuite()`. Do not recreate shared resources per test.
- **Isolate slow tests:** Consider disabling or isolating flaky or long-running tests, running them separately or conditionally.
- **Limit dependency chains:** Tests should not depend on each other, which can cause serial bottlenecks.

### 4. Use of Test Fixtures Smartly

Reusing test fixtures optimally can reduce redundant setup.

- Implement `SetUpTestSuite()` and `TearDownTestSuite()` for shared expensive setup.
- Avoid heavy computations in `SetUp()` or `TearDown()` per test when possible.
- Use parameterized tests (`TEST_P`) to share logic over multiple input sets without duplicating setup.

### 5. Employ Test Event Listeners for Custom Reporting

Logging and reporting can introduce overhead.

- Use GoogleTest’s event listener API to configure lightweight output during extensive test runs.
- Consider custom listeners that suppress verbose output or log only failures to speed up feedback.

---

## Example: Custom Minimal Test Output for Faster Reporting

You can simplify test output to reduce I/O overhead and speed up test evaluation using a custom event listener, as shown below:

```cpp
#include "gtest/gtest.h"

using ::testing::EmptyTestEventListener;
using ::testing::TestEventListeners;
using ::testing::UnitTest;

class TersePrinter : public EmptyTestEventListener {
 private:
  void OnTestProgramEnd(const UnitTest& unit_test) override {
    printf("TEST %s\n", unit_test.Passed() ? "PASSED" : "FAILED");
    fflush(stdout);
  }
  void OnTestStart(const testing::TestInfo& test_info) override {
    printf("*** Test %s.%s starting.\n", test_info.test_suite_name(), test_info.name());
    fflush(stdout);
  }
  void OnTestPartResult(const testing::TestPartResult& test_part_result) override {
    printf("%s in %s:%d\n%s\n",
           test_part_result.failed() ? "*** Failure" : "Success",
           test_part_result.file_name(),
           test_part_result.line_number(),
           test_part_result.summary());
    fflush(stdout);
  }
  void OnTestEnd(const testing::TestInfo& test_info) override {
    printf("*** Test %s.%s ending.\n", test_info.test_suite_name(), test_info.name());
    fflush(stdout);
  }
};

int main(int argc, char** argv) {
  ::testing::InitGoogleTest(&argc, argv);

  bool terse_output = argc > 1 && strcmp(argv[1], "--terse_output") == 0;

  UnitTest& unit_test = *UnitTest::GetInstance();

  if (terse_output) {
    TestEventListeners& listeners = unit_test.listeners();
    delete listeners.Release(listeners.default_result_printer());
    listeners.Append(new TersePrinter);
  }

  return RUN_ALL_TESTS();
}
```

Run your test program with `--terse_output` to enable this streamlined output.

---

## Practical Tips & Best Practices

- **Profile your tests** to identify slow tests and setup costs.
- **Run tests locally in filtered subsets** during development to get feedback fast.
- **Leverage CI infrastructure features** like parallel builds and test shards.
- **Design tests for isolation and minimal dependency** to enable maximum parallelism.
- **Use parameterized and typed tests** to maximize reuse and minimize code duplication.

---

## Troubleshooting Common Performance Issues

- **Tests run too slowly overall:** Check for expensive per-test or per-fixture initialization. Use suite-level setup instead.
- **Tests run serially despite multiple cores:** Verify your CI or test runner is configured to run tests or shards in parallel.
- **Tests failing due to shared mutable state:** Ensure fixtures properly isolate tests or protect shared state to avoid race conditions during parallel runs.
- **Excessive logging slows tests:** Use filtering or custom event listeners to reduce console output.

---

## Continuous Integration Considerations

To maintain fast and reliable CI:

- Use sharding to spread tests across multiple CI workers.
- Combine sharding with parallel test jobs for maximal test throughput.
- Generate machine-readable test reports (XML/JSON) for automated result analysis.
- Integrate test timeout mechanisms to detect and fail hanging tests early.

For implementation guidance, see the [Scaling Tests and Integrating with CI](https://github.com/google/googletest/blob/main/guides/advanced-and-real-world/scaling-and-integration.md) guide.

---

## Additional Resources

- [GoogleTest Primer](primer.md) — Introduction and core testing concepts
- [Test Discovery and Execution Lifecycle](concepts/execution-integration-behavior/test-discovery-and-lifecycle.md) — How tests are run internally
- [Scaling Tests and Integrating with CI](guides/advanced-and-real-world/scaling-and-integration.md) — Strategies to scale tests
- [Test Macros & Fixtures Reference](api-reference/core-testing-apis/test-macros.md)

---

Keep your test suite fast and reliable by adopting these test selection, parallelization, and setup practices. Doing so ensures your continuous integration pipeline delivers timely feedback, supporting rapid development and high-quality code.

---

_This page focuses specifically on optimizing test execution speed and throughput with GoogleTest. For broader context on the GoogleTest architecture and ecosystem, refer to [System Architecture Overview](overview/core-architecture-features/architecture-overview.md) and [Supported Platforms](overview/integration-and-ecosystem/supported-platforms.md)._