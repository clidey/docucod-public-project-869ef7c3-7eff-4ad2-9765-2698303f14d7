---
title: "Assertion Failures & Test Output"
description: "Explains how to interpret and debug assertion failures in test output. Covers meaning of common assertion error messages and offers step-by-step suggestions for diagnosing mismatches and understanding test results."
---

# Assertion Failures & Test Output

When you run your tests with GoogleTest, the output you see often includes details of assertion failures when a test does not pass. Understanding these failure messages is essential to diagnose problems quickly and efficiently. This guide explains how to interpret common GoogleTest assertion failure messages, what they mean, and how to use them to debug your tests.

---

## 1. Understanding Assertion Failures

At its core, GoogleTest assertions check conditions or expressions to verify the correctness of your code. When an assertion fails, GoogleTest prints detailed information about what was expected, what was actually found, and where the failure occurred.

### Key Components of a Failure Message

- **Location**: Source file and line number where the failure happened.
- **Expected Condition**: What GoogleTest expected to be true.
- **Actual Value**: The value or result encountered in the test.
- **Failure Type**: Fatal failures stop the current test immediately, while non-fatal failures allow the test to continue.
- **Stack Trace** (if enabled): Helps trace back the call path leading to failure.

### Example Failure Output

```none
example_test.cc:42: Failure
Expected equality of these values:
  foo
    Which is: 5
  bar
    Which is: 6
```

This indicates that at `example_test.cc` line 42, a test expected `foo == bar` but found `foo = 5` and `bar = 6`.

---

## 2. Common Assertion Failure Messages Explained

GoogleTest offers a rich set of assertions. Below are examples of common failure messages and what they mean:

### Equality Failures (e.g., `EXPECT_EQ`, `ASSERT_EQ`)

```
Expected equality of these values:
  var1
    Which is: 10
  var2
    Which is: 20
```

- Means the two values compared were expected to be equal but differed.
- Useful for numeric or string comparisons.

### Boolean Assertions (e.g., `EXPECT_TRUE`, `ASSERT_FALSE`)

```
Value of: is_ready
  Actual: false
Expected: true
```

- Indicates a boolean condition was expected to be true but was false, or vice versa.

### String Comparison Failures (e.g., `EXPECT_STREQ`, `EXPECT_STRNE`)

```
Expected equality of these strings:
  "hello"
  "world"
```

- Shows a mismatch when comparing C strings.
- Note that pointer equality is different than string content equality; use the correct macros.

### Floating-Point Assertions (e.g., `EXPECT_FLOAT_EQ`, `EXPECT_NEAR`)

```
The difference between val1 and val2 is 0.5, which exceeds 0.4
```

- Indicates a violation of approximate equality within a defined tolerance.

### Exception Assertions (e.g., `EXPECT_THROW`, `ASSERT_NO_THROW`)

```
Expected: function() throws an exception of type std::runtime_error.
  Actual: it throws nothing.
```

- Confirms that expected exceptions were or were not thrown.

### Death Test Failures

```
Expected: statement dies with exit code non-zero and stderr matching "Error.*"
```

- Shows that a death test expected a process to crash under certain conditions.

---

## 3. Diagnosing Assertion Failures

When faced with an assertion failure, follow these steps:

### Step 1: Identify the Failure Location

The output always includes the source filename and line number.
Check this location in your code to see which assertion failed.

### Step 2: Understand the Expected vs Actual Values

- Look at the "Expected:" and "Actual:" sections.
- For comparison failures, check the differing values.
- For boolean checks, verify why the condition failed.

### Step 3: Review the Stack Trace (If Present)

- The stack trace shows the sequence of function calls leading to the failure.
- Can be enabled or disabled via the `--gtest_stack_trace_depth` flag.
- Use it to trace how the failing value propagated.

### Step 4: Rerun with More Verbose Logging

- Use `SCOPED_TRACE()` in your tests to add contextual messages.
- Provide custom failure messages streaming to assertions.

### Step 5: Use Predicate or Predicate-Format Assertions

- When simple assertions donâ€™t give enough info, define predicate functions returning `AssertionResult`.
- Use `EXPECT_PRED_FORMAT*` to customize failure messages and improve diagnosis.

---

## 4. Practical Examples of Common Failure Scenarios

### Mixing TEST() and TEST_F() in the Same Test Suite

```
All tests in the same test suite must use the same test fixture class, so mixing TEST_F and TEST in the same test suite is illegal.
```

- Occurs if different test fixture classes are used in the same suite.
- Fix by using consistent fixture classes or splitting tests into separate suites.

### Parameterized Tests Failures with Missing Instantiations

```
Parameterized test suite FooTest is defined via TEST_P, but never instantiated.
None of the test cases will run.
```

- Happens when `TEST_P` is defined but `INSTANTIATE_TEST_SUITE_P` is not provided.
- Resolve by ensuring correct instantiation of parameterized tests.

### Assertion in a Fixture Constructor or Destructor

```
Expected failure #1, in the test fixture c'tor.
```

- Using fatal assertions in constructors or destructors isn't supported.
- Use `SetUp()` or `TearDown()` for assertions.

### Failure on String Comparison with Embedded Null Characters

```
Expected equality of these values:
  "st\0ring"
  "st\0ring"
```

- GoogleTest can compare strings with embedded `\0` shown as escaped.

### Failures in Death Tests

When a death test fails to die or outputs unexpected message:

```
Expected: statement dies with exit code non-zero and stderr matching "Some error"
Actual: process exited normally.
```

- Adjust death test code or matcher patterns accordingly.

---

## 5. Tips and Best Practices for Effective Debugging

- **Use Custom Messages:** Add failure messages with a streaming operator to explain the cause.
  ```cpp
  EXPECT_EQ(value, expected) << "Value mismatch in processing input " << input_data;
  ```
- **Leverage SCOPED_TRACE:** Add linked traces to identify failure contexts, especially in loops or subroutines.
- **Use Predicate Assertions:** For complex conditions, write custom predicates returning `AssertionResult` for better diagnostics.
- **Keep Tests Isolated:** Avoid shared mutable state causing flaky failures.
- **Check for Disabled Tests:** Ensure failing tests are not disabled accidentally.
- **Run Tests with --gtest_break_on_failure:** If available, break on failures within a debugger.

---

## 6. Enabling and Controlling Stack Traces

- Stack traces help reveal where and how an assertion failed.
- Control depth via command line flag `--gtest_stack_trace_depth=N`.
- May require platform support (e.g., Abseil Stacktrace on linux).
- Internal stack frames may be elided by default to focus on user code.

---

## 7. Test Output Formats and Filters

GoogleTest supports output in multiple forms to help in interpreting results:

- **Console Output:** Real-time results including progress and failures.
- **XML Output:** Use `--gtest_output=xml[:file_path]` to generate detailed reports useful for automation.
- **JSON Output:** Use `--gtest_output=json[:file_path]` for an alternative machine-readable format.
- **Streaming Output:** Use `--gtest_stream_result_to=host:port` to send results over the network.

Filtering and running subsets of tests is done via `--gtest_filter` command line/ environment variable. Filters control which tests run and which are reported.

---

## 8. Handling Flaky and Unexpected Failures

- Use `--gtest_repeat` to rerun tests multiple times to detect instability.
- Use `--gtest_shuffle` to randomize test execution order and uncover ordering dependencies.
- Add proper setup/teardown to manage test environment consistency.

---

## 9. Troubleshooting Common Pitfalls

<AccordionGroup title="Troubleshooting Common Assertion Failures">
<Accordion title="Failure Due to Mixing Fixtures in One Test Suite">
Mixing different test fixture classes in the same test suite causes failures. Ensure all tests in a suite use the same fixture class.
</Accordion>
<Accordion title="Failure Because of Missing Test Instantiation">
When using parameterized or typed tests, lack of proper instantiation triggers failures. Verify you have included corresponding `INSTANTIATE_TEST_SUITE_P` or `INSTANTIATE_TYPED_TEST_SUITE_P` macros.
</Accordion>
<Accordion title="Failures in Constructors or Destructors">
Avoid fatal assertion macros like `FAIL()` or `ASSERT_*` in constructor or destructor. Use `SetUp()` and `TearDown()` methods for such checks.
</Accordion>
<Accordion title="String Comparison Failures with Embedded Nulls">
GoogleTest shows embedded null characters as escaped. Consider using matchers for complex or binary string comparisons.
</Accordion>
<Accordion title="Death Test Failures Not Dying as Expected">
Check the death test macro and regex for stderr carefully. Also verify platform support and test conditions.
</Accordion>
</AccordionGroup>

---

## 10. Getting More Help

If you need further assistance interpreting test failures or test output:

- Use `EXPECT_FATAL_FAILURE()` and `EXPECT_NONFATAL_FAILURE()` macros to write tests on your tests.
- Consult the [Assertions Reference](reference/assertions.md) for detailed assertion information.
- Review the [Advanced GoogleTest Topics](docs/advanced.md) for expert usage.
- Ask in community forums or search issues in the official GitHub repo.

---

## See Also

- [Writing and Running Tests](api-reference/core-testing-apis/writing-tests.md)
- [Assertions Reference](docs/reference/assertions.md)
- [Using Assertions Effectively](guides/real-world-usage-and-best-practices/using-assertions-effectively.mdx)
- [Running Tests and Interpreting Results](guides/getting-started-core-workflows/running-tests-and-interpreting-results.mdx)
- [Writing and Using Death Tests](guides/performance-and-specialized-use-cases/writing-and-using-death-tests.mdx)

---

## Summary

This documentation page provides comprehensive explanations on how to interpret GoogleTest assertion failure outputs and guides users through pragmatic steps to diagnose and debug test failures effectively. It highlights various failure types, suggested troubleshooting practices, best practices for adding clarity to tests, and ways to control test output for better insights.

---

### Code Example: Using SCOPED_TRACE for Better Failure Context

```cpp
void FunctionUnderTest(int n) {
  SCOPED_TRACE(testing::Message() << "n = " << n);
  EXPECT_EQ(n, 5);
}

TEST(MyTestSuite, TestWithTrace) {
  for (int i = 0; i < 10; ++i) {
    FunctionUnderTest(i);  // Failure message will show which 'i' failed
  }
}
```

### Code Example: Custom Predicate Assertion

```cpp
// Custom predicate that returns AssertionResult
::testing::AssertionResult IsPositive(int n) {
  if (n > 0) return ::testing::AssertionSuccess();
  return ::testing::AssertionFailure() << n << " is not positive";
}

TEST(MyTestSuite, PositiveCheck) {
  EXPECT_PRED1(IsPositive, value);
}
```

---

With this knowledge, you can confidently interpret and act upon assertion failures in your test output, accelerating your debugging and improving code quality.
