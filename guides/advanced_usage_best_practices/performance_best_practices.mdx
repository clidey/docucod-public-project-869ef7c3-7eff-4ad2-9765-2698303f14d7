---
title: "Performance and Maintainability Best Practices"
description: "Learn patterns for organizing fast, reliable, and maintainable test suites. This guide highlights common pitfalls, techniques for parallel test execution, and recommendations for reducing test flakiness and improving feedback cycles."
---

# Performance and Maintainability Best Practices

## Overview

This guide helps you design and organize **fast, reliable, and maintainable** test suites using GoogleTest and GoogleMock. You will learn proven patterns, techniques to avoid common pitfalls, approaches to parallelize test execution, and recommendations to reduce flakiness and improve your feedback cycle.

By following these best practices, your tests will complete quickly, provide clear and immediate feedback, and remain robust and easy to update as your code evolves.

---

## 1. Structuring Tests for Speed and Reliability

### 1.1 Write Independent, Small Tests

- **Goal:** Each test verifies one piece of behavior in isolation.
- **Benefits:** Faster failure localization and reduced maintenance overhead.

**Best Practices:**
- Avoid coupling tests via shared state.
- Limit expensive setup to only what the test requires.
- Use test fixtures (`TEST_F`) to reduce code duplication without introducing hidden dependencies.

### 1.2 Use Test Fixtures Wisely

Test fixtures allow sharing common setup and teardown code. To keep tests maintainable:

- Avoid storing modifiable shared data in fixtures unless necessary.
- Reset or reinitialize shared data in `SetUp()` before each test to prevent test interference.
- Prefer immutable fixtures or use per-test setup to avoid stale state.

---

## 2. Reducing Flaky Tests

Flaky tests fail non-deterministically, slowing development. Hereâ€™s how to avoid them:

### 2.1 Control External Dependencies

- **Mock external dependencies** using GoogleMock to isolate system behavior and remove reliance on databases, networks, or file systems.
- Use *fakes* or *stubs* when interactions with real services are too costly or unreliable.

### 2.2 Specify Expectations Precisely but Flexibly

- Avoid overspecifying call counts, argument values, or call order unless necessary.
- Use matchers like `_` when argument values do not affect test correctness.
- Use cardinalities like `AtLeast(n)` or `AnyNumber()` to allow legitimate variation.

### 2.3 Order Expectations Only When Necessary

- Avoid strict sequencing of mocks unless the tested logic depends on call order.
- Use `InSequence` only to verify essential call order, preventing test brittleness.

### 2.4 Clean Resource Management

- Prefer stack allocation for mocks and test objects to ensure lifetime management.
- Use heap checkers and verify no resource leaks occur, especially with mocks that require virtual destructors.

---

## 3. Parallelizing Test Execution

Maximize CPU utilization and minimize wall clock time by running tests concurrently.

### 3.1 Isolate Tests to Facilitate Parallelism

- Ensure that tests do not share mutable global state or files unless properly synchronized.
- Avoid side effects on external resources unless they are uniquely assigned per test.

### 3.2 Use Test Sharding

- GoogleTest supports test sharding through environment variables `GTEST_TOTAL_SHARDS` and `GTEST_SHARD_INDEX`:
  - Set `GTEST_TOTAL_SHARDS` to the total number of parallel shards.
  - Set `GTEST_SHARD_INDEX` uniquely per shard.
- Each shard runs a subset of tests, ensuring complete coverage across shards.

### 3.3 Design Fixtures and Environments for Parallel Safety

- Use `SetUpTestSuite()` and `TearDownTestSuite()` for expensive global setup, ensuring the code is thread-safe or used only in single-threaded contexts.
- Use `::testing::Environment` to set global shared resources carefully.

---

## 4. Improving Feedback Cycles

Fast feedback helps catch regressions early and makes debugging efficient.

### 4.1 Selective Test Execution

- Use `--gtest_filter` to focus on failed or related tests during development.
- Disable flaky or slow tests temporarily with `DISABLED_` prefixes, but schedule fixes promptly.

### 4.2 Use Assertions Effectively

- Use `ASSERT_*` for fatal checks that must stop the test immediately.
- Use `EXPECT_*` to collect multiple failures per test run, providing richer insights.

### 4.3 Leverage GoogleTest Features

- Use `SCOPED_TRACE` to add context to assertions within helper functions.
- Use `--gtest_repeat` for flaky tests to increase test reliability.

---

## 5. Maintainability Recommendations

### 5.1 Write Clear, Concise Tests

- Name test suites and cases clearly to reflect their purpose.
- Limit test code complexity; extract helpers for repetitive or complex test logic.

### 5.2 Use Parameterized and Typed Tests

- Parameterize tests to cover multiple input scenarios with less code and duplication.
- Use typed tests to verify behavior over a range of types.

### 5.3 Organize Mocks and Helpers

- Organize mocks in logical namespaces or files.
- Avoid mocking classes you do not own unless absolutely necessary.
- Delegate to fakes or real objects to minimize mock maintenance.

### 5.4 Use `NiceMock` and `StrictMock` Appropriately

- `NiceMock`: suppress warnings on uninteresting calls, useful during early development.
- `StrictMock`: treat uninteresting calls as failures, suitable for final verification.
- Avoid mixing types to prevent confusing test behaviors.

---

## 6. Common Pitfalls

- **Not specifying virtual destructors on interfaces:** causes leaks and heapcheck failures with mocks.
- **Expectations set after code under test is exercised:** leads to undefined behavior.
- **Overly strict expectations:** cause brittle tests that fail on refactoring.
- **Ignoring proper sequencing of calls:** can mask bugs or cause false positives.

---

## 7. Practical Tips & Suggestions

- Set up **default actions** using `ON_CALL` for common behaviors, reserve `EXPECT_CALL` for verified interactions.
- Use **matchers** to specify only critical argument conditions.
- Use **`.RetiresOnSaturation()`** when multiple expectations overlap to avoid confusion.
- Turn on verbose mode (`--gmock_verbose=info`) to debug expectation mismatches.
- Use **heap checkers** and enable **mock leak detection** for test robustness.

---

## 8. Example: Organizing Tests for a Graphics Module

Suppose you have a graphics rendering module.

1. Define interfaces for device APIs (e.g., `Turtle` interface).
2. Create mocks for the interfaces with `MOCK_METHOD` macros, including virtual destructors.
3. Write clear, independent tests for drawing primitives, verifying interaction with the mock.
4. Use sequences only where drawing order is critical.
5. Use parameterized tests for different shapes and sizes.
6. Use `ON_CALL` to set default behavior such as ignoring uninteresting calls.
7. Run tests in parallel shards by isolating graphics contexts.

This ensures fast, maintainable, and reliable graphics testing.

---

## 9. Troubleshooting Common Performance and Maintainability Issues

### Problem: Tests Are Too Slow

- **Cause:** Expensive setup or real resource dependencies.
- **Solutions:** Mock external dependencies, minimize fixture setup, parallelize tests.

### Problem: Flaky Tests

- **Cause:** Timing, order dependencies, non-deterministic external factors.
- **Solutions:** Add mocks, relax strict expectations, use retries, isolate global state.

### Problem: Tests Break After Refactoring

- **Cause:** Overly strict expectations or brittle mocks.
- **Solutions:** Use flexible matchers, avoid testing implementation details, refactor tests with code.

### Problem: Mock Leak or Heap Check Failures

- **Cause:** Missing virtual destructors, unmanaged mock lifetimes.
- **Solutions:** Ensure virtual destructors, use RAII or smart pointers for mocks, enable heap checks.

---

## 10. Next Steps & Related Content

- Master **Effective Mocking and Expectation Setting** in the core testing patterns guides.
- Explore **Advanced Usage and Best Practices** for death tests and CI integration.
- Consult the **Mocking Reference** for detailed API descriptions.
- Use the **gMock Cookbook** for recipe-style guidance on complex mocking scenarios.

---

## References

- [GoogleTest Primer](primer.md)
- [Effective Mocking and Expectation Setting](guides/core_testing_patterns/effective_mocking.md)
- [Mocking Reference](docs/reference/mocking.md)
- [gMock Cookbook](gmock_cook_book.md)
- [Performance Optimization FAQ](faq/troubleshooting-optimization/performance-optimization-faq.md)

---

## Diagram: Test Execution and Expectation Workflow

```mermaid
flowchart TD

  Start([Start Test Suite]) --> SetupFixtures["Setup Test Fixtures"]
  SetupFixtures --> DefineMocks["Define Mock Objects"]
  DefineMocks --> SetDefaults["ON_CALL: Set Default Behaviors"]

  SetDefaults --> SetExpectations["EXPECT_CALL: Set Verified Expectations"]
  SetExpectations --> ExerciseCode["Exercise Code Under Test"]
  ExerciseCode --> VerifyCallOrder{Use InSequence?}

  VerifyCallOrder -->|Yes| EnsureOrder["Verify Calls In Sequence"]
  VerifyCallOrder -->|No| AllowAnyOrder["Allow Any Call Order"]

  EnsureOrder --> ValidateExpectations["Validate Expectations"]
  AllowAnyOrder --> ValidateExpectations

  ValidateExpectations --> Cleanup[
    "TearDown Fixtures and Mocks"
  ]
  Cleanup --> End([End Test Suite])

  %% Parallel Execution branch
  Start -.-> ParallelEnv["Environment Setup for Parallel Test Shards"]
  ParallelEnv -.-> SetupFixtures

  %% Failure branch
  ValidateExpectations -->|Fail| ReportFailure["Report Failure & Provide Diagnostics"]
  ReportFailure --> Cleanup

  classDef decision fill:#f9f,stroke:#333,stroke-width:2px;
  class VerifyCallOrder decision;
```
