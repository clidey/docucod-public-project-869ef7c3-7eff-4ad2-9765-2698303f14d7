---
title: "Integrating with Continuous Integration (CI)"
description: "Explains how to run GoogleTest as part of CI pipelines, generate machine-readable output, and troubleshoot CI test failures. Covers typical configurations and platform-specific advice."
---

# Integrating with Continuous Integration (CI)

## Workflow Overview

GoogleTest is designed to seamlessly integrate into Continuous Integration (CI) pipelines, enabling automated execution of tests with machine-readable output. This guide helps you configure GoogleTest for CI environments, generate parsable test reports, handle typical platform-specific setups, and troubleshoot common CI-related issues.

### Prerequisites

- A working GoogleTest build of your test binaries.
- Basic familiarity with running tests manually from the command line.
- Access to your CI environment's configuration (e.g., Jenkins, GitHub Actions, GitLab CI).

### Expected Outcome

By following this guide, you will successfully:

- Execute GoogleTest binaries as part of automated CI runs.
- Generate output in formats compatible with CI reporting tools.
- Detect and diagnose common causes of CI test failures.
- Apply platform-appropriate configurations for reliable test execution.

### Time Estimate

15-30 minutes depending on your CI system and project complexity.

### Difficulty Level

Intermediate â€” requires understanding of both GoogleTest execution and CI pipeline configuration.

---

## Step-by-Step Instructions

<Steps>
<Step title="Run GoogleTest in CI">
Execute your compiled GoogleTest binary within your CI job script as you would locally. Because GoogleTest returns an exit code indicative of success or failure, the CI system can detect test pass/fail status reliably.

Example invocation:

```bash
./my_test_binary
```

GoogleTest's return codes:

- 0: All tests passed.
- 1: One or more tests failed or other test failures.

<Check>
Always ensure your CI job captures and checks your test binary's exit code to decide pass/fail status.
</Check>

</Step>

<Step title="Enable Machine-Readable Output">
To integrate with CI tools that parse test reports, use GoogleTest's built-in XML output feature for JUnit-style reports.

Add the flag:

```bash
--gtest_output=xml:<path_to_report>/report.xml
```

Example:

```bash
./my_test_binary --gtest_output=xml:reports/test_results.xml
```

The generated XML file can be consumed by most CI platforms' test report viewers.

<Tip>
Set up your CI environment to archive or upload the generated report.xml for analysis.
</Tip>

</Step>

<Step title="Filter and Select Tests for CI Runs">
To control which tests run in your CI pipeline, use GoogleTest's filtering flag:

```bash
--gtest_filter=<test_suite_name>.<test_name>
```

Supports wildcards (`*`, `-`) to include or exclude tests.

Example: Run all tests except slow integration tests:

```bash
./my_test_binary --gtest_filter=-*IntegrationTest.*
```

<Note>
Use test filtering to minimize CI runtime while focusing on critical tests.
</Note>

</Step>

<Step title="Customize Test Behavior for CI">
To improve CI reliability and diagnosing failures, leverage GoogleTest flags:

- `--gtest_repeat=N` to repeat tests multiple times, useful for catching flaky tests.
- `--gtest_shuffle` to run tests in a random order, detecting order dependencies.
- `--gtest_break_on_failure` to pause execution immediately on first test failure (useful in debugging).

Example:

```bash
./my_test_binary --gtest_repeat=3 --gtest_shuffle
```

</Step>

<Step title="Address Platform-Specific Considerations">
Different OS environments may require slight configuration changes:

- **Linux/macOS**: Ensure shared libraries (like pthread if applicable) are found correctly.
- **Windows**: Use the gtest_main or gmock_main libraries to simplify main() handling. Be cautious about runtime library consistency (static vs dynamic).

See [Installation (All Platforms)](/getting-started/essential-setup/installation-multiple-platforms) for detailed build guidance per OS.

</Step>

<Step title="Troubleshoot CI Test Failures">
Common issues in CI integration include:

- **Test discovery failures**: Verify tests are properly linked and available in the binary.
- **Linker/runtime mismatches**: On Windows, runtime library mismatches can cause failures.
- **Uninteresting mock calls warnings/failures**: Use `NiceMock` or `StrictMock` appropriately to manage mock call strictness.
- **Permissions and environment variables**: Ensure the CI environment can execute tests and read/write output files.

Use verbose output flags for diagnosis:

```bash
./my_test_binary --gtest_verbose=info
```

Refer also to the [Troubleshooting & Common Issues](/getting-started/quickstart-and-troubleshooting/troubleshooting-validation) page.

</Step>

<Step title="Verify Successful CI Integration">
Confirm tests execute, generate results, and that your CI platform processes the XML report correctly.

Typical Success Indicators:

- CI job exits with code 0 when tests pass.
- Collect and display of test reports from generated XML.
- No unexpected flakiness or environment-caused failures.

</Step>

</Steps>

---

## Examples & Usage

### Example CI Command

A typical CI shell snippet for running tests with reports:

```bash
mkdir -p reports
./my_test_binary --gtest_output=xml:reports/test_results.xml
```

### Example Filtering in CI

Run only fast unit tests, avoiding integration ones:

```bash
./my_test_binary --gtest_filter=*-*IntegrationTest.*
```

### Example Integration with Jenkins (Declarative Pipeline)

```groovy
pipeline {
  agent any
  stages {
    stage('Build') {
      steps {
        sh 'cmake --build build --config Release'
      }
    }
    stage('Test') {
      steps {
        sh './build/my_test_binary --gtest_output=xml:reports/test_results.xml'
        junit 'reports/*.xml'
      }
    }
  }
}
```

---

## Troubleshooting & Tips

<AccordionGroup title="Common CI Integration Issues">
<Accordion title="Tests Not Running or Found in CI">
- Verify test binaries are built and executable in the CI environment.
- Check if necessary runtime dependencies (shared libs) are installed or packaged.
- Confirm no environment variables differ that may disable tests.
</Accordion>

<Accordion title="Unexpected Test Failures on CI but Not Locally">
- Flaky tests may fail due to timing or resource constraints.
- Use `--gtest_repeat` and `--gtest_shuffle` locally to reproduce order or timing issues.
- Add detailed logs or `--gtest_break_on_failure` locally.
</Accordion>

<Accordion title="XML Report Not Detected by CI System">
- Confirm the `--gtest_output=xml:` path is in the directory your CI expects.
- Check file permissions and path validity.
- Enable verbose logging `--gtest_verbose=info` for hints.
</Accordion>

<Accordion title="Mock-Related Runtime Failures in CI">
- If unexpected or uninteresting mock calls cause failings, try wrapping mocks with `NiceMock`.
- Use GoogleMock's diagnostics to identify mock mismatches.
</Accordion>

</AccordionGroup>

<Tip>
Incorporate test output archiving or publishing steps in your CI configuration to preserve test history.
</Tip>

---

## Next Steps & Related Documentation

- Explore [Installation and Setup](/guides/getting-started/install-and-setup) to ensure your environment supports CI builds.
- Review [Organizing and Running Tests](/guides/getting-started/organizing-tests) to better structure test suites for CI.
- See [Debugging Test Failures and Output Customization](/guides/integration-and-best-practices/debugging-and-output) for advanced troubleshooting.
- For mocking users, consult [Setting Expectations and Verifying Interactions](/guides/mocking-in-action/setting-expectations) to manage mock-related CI errors.
- Consult your CI platform's documentation on JUnit XML report integration.

---