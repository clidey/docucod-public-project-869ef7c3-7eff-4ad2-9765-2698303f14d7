---
title: "Assertion Model and Failure Semantics"
description: "Explore how assertions are used to drive test outcomes, including the distinction between fatal and non-fatal failures. See how these design choices influence test robustness and facilitate clear, actionable reporting."
---

# Assertion Model and Failure Semantics

Explore how assertions are used to drive test outcomes, including the distinction between fatal and non-fatal failures. See how these design choices influence test robustness and facilitate clear, actionable reporting.

---

## Introduction

Assertions are the fundamental mechanism through which GoogleTest verifies correctness in your code. They express expectations or checks that must hold true for your tests to pass. Understanding the assertion model, including the difference between fatal and non-fatal failures, empowers you to write robust tests that yield clear, actionable outcomes.

This guide focuses solely on **assertions and their failure semantics** within GoogleTest, providing you with the knowledge to effectively incorporate them into your testing workflows.

---

## The Role of Assertions in Testing

At the heart of each test, assertions validate conditions that your code must satisfy. When an assertion fails, GoogleTest records the failure and reports it at the conclusion of the test run. This mechanism ensures you are informed about precisely which checks did not meet expectations.

### Two Failure Types:

Assertions in GoogleTest come paired, differentiated by their failure severity and impact on test execution:

| Assertion Macro Prefix | Failure Type | Test Execution Impact                          |
| --------------------- | ------------ | --------------------------------------------- |
| `ASSERT_`              | Fatal        | Aborts the current function immediately       |
| `EXPECT_`              | Non-fatal    | Continues running current function despite failure |


### Why Fatal vs. Non-fatal?

- **Fatal failures** (`ASSERT_`) immediately end the test function, preventing further execution when continuing wouldn't make sense (e.g., when a crucial prerequisite isn't met).
- **Non-fatal failures** (`EXPECT_`) allow the test to continue, capturing multiple related failures in a single run to aid debugging.

Choose the appropriate kind based on whether subsequent test steps rely on the success of the assertion.

---

## Detailed Assertion Behaviors

### Fatal Failures (`ASSERT_` Macros)

- Stop execution of the current test function immediately.
- Mark the test as failed.
- Prevent further code in the test function from running, which is critical where continued execution would cause undefined behavior or misleading results.
- Example: `ASSERT_TRUE(ptr != nullptr);` ensures that dereferencing `ptr` is safe thereafter.

### Non-fatal Failures (`EXPECT_` Macros)

- Report the failure but do not abort the current test function.
- Allow subsequent assertions/checks in the test to run, helping to identify multiple issues before test completion.
- Tests are marked as failed if any non-fatal failure occurs.
- Use when the test can meaningfully continue despite failure of the assertion.

---

## Writing and Using Assertions

### Basic Usage

All assertion macros accept the expression or condition to verify. You can optionally append a custom failure message using stream operators for enhanced reporting clarity.

```cpp
ASSERT_TRUE(value > 0) << "Value must be positive";
EXPECT_EQ(expected, actual) << "Values differ";
```

### Explicit Success and Failure

GoogleTest provides macros to explicitly generate success or failure results without testing expressions:

| Macro                  | Behavior                             | Notes                            |
| ---------------------- | ---------------------------------- | --------------------------------|
| `SUCCEED()`            | Generates a successful assertion   | Does not guarantee overall test success; just documents checks passed |
| `FAIL()`               | Generates a fatal failure           | Immediately aborts current void function |
| `ADD_FAILURE()`         | Generates a non-fatal failure       | Allows current function to continue |
| `ADD_FAILURE_AT(file, line)` | Generates a non-fatal failure at specified location | Useful for reporting failures at custom source locations |

These are especially handy when control flow, not an expression, determines success.

### Examples

```cpp
// Abort if mode is invalid.
ASSERT_TRUE(mode == 0 || mode == 1) << "Invalid mode value";

// Continue, but record failure if sizes differ.
EXPECT_EQ(expected_size, actual_size) << "Size mismatch";

// Explicit failure in default branch.
switch(input) {
  case 1:
    // ... checks ...
    break;
  default:
    FAIL() << "Unexpected input value";
}

// Mark a certain code path as reaching success.
SUCCEED() << "Reached expected execution path";
```

---

## Assertion Variants and Use Cases

GoogleTest supplies numerous assertion macros. The general naming scheme is:

- `EXPECT_*` for non-fatal assertions
- `ASSERT_*` for fatal assertions

Some categories include:

- Boolean Condition Assertions (`EXPECT_TRUE`, `ASSERT_FALSE`, etc.)
- Equality and Relational Assertions (`EXPECT_EQ`, `ASSERT_LT`, etc.)
- String Equality Assertions (`EXPECT_STREQ`, `ASSERT_STRCASEEQ`, etc.)
- Floating-Point Close Comparisons (`EXPECT_FLOAT_EQ`, `ASSERT_DOUBLE_EQ`)
- Exception Assertions (`EXPECT_THROW`, `ASSERT_NO_THROW`, etc.)
- Predicate Assertions with custom predicates
- Generalized Assertions using Matchers (`EXPECT_THAT`)

For detail on all assertion macros and their typical usages, see the [Assertions Reference](../api-reference/core-test-api/assertions-reference).

---

## Streaming Custom Failure Messages

You can add custom failure messages to assertions with the `<<` operator, which streams content to be appended to the failure output.

```cpp
EXPECT_EQ(foo, bar) << "Expected foo and bar to be equal because of XYZ";
```

This helps document the intent behind the failure and speeds diagnosis.

---

## How Assertions Affect Test Flow

### Fatal Failures Impact

- Immediately terminate the current function (test or helper) they are in.
- Prevent code after the assertion from running, which protects against false positives or crashes due to an invalid test state.

### Non-fatal Failures Impact

- Record failure but let the current function continue.
- Help detect multiple failing conditions in one test run, improving debugging efficiency.

Use fatal assertions when failure means subsequent code is invalid or dangerous to execute.

Use non-fatal assertions when testing multiple independent checks or conditions where it is beneficial to observe multiple failures simultaneously.

---

## Best Practices for Using Assertions

- Be mindful about choosing between `ASSERT_` (fatal) and `EXPECT_` (non-fatal) based on whether continuing test execution after failure is safe and meaningful.
- Add clear, descriptive custom messages to improve failure diagnostics.
- Avoid putting complex logic that might affect side effects inside assertions.
- Use `EXPECT_THAT` with matchers for expressive and flexible conditions.
- In death tests or code verified by multiple threads, use appropriate fatal assertions to catch critical failures.

---

## Understanding Failure Reporting

When an assertion fails, GoogleTest:

- Records detailed information including the expression source, file, and line number.
- Prints a failure message describing the condition that failed.
- Shows any streamed messages.
- Continues or aborts as specified.
- Reports cumulative test results after test execution finishes.

This design helps pinpoint failure causes quickly and accurately.

---

## Troubleshooting Assertions

### Common Pitfalls

- Using fatal assertions (`ASSERT_`) in helper functions intended to continue test execution can cause unintended test abort.
- Using `EXPECT_` where a failure invalidates further testing logic may cause cascading errors and obscure root cause.
- Forgetting to add meaningful failure messages can make debugging harder.

### Tips

- Use `ASSERT_` when subsequent code depends on the assertion passing.
- Use `EXPECT_` for checks where independent validation is desired.
- Run with verbose output or debug mode to inspect failure locations and messages.

---

## Additional Helpful Macros

| Macro                   | Description                                                              |
| ----------------------- | -------------------------------------------------------------------------|
| `EXPECT_FATAL_FAILURE(statement, substring)` | Expects `statement` will cause a fatal failure containing `substring`. Useful for testing failure conditions. |
| `EXPECT_NONFATAL_FAILURE(statement, substring)` | Expects non-fatal failure with `substring`. Useful to verify soft assertion failure handling. |

These are typically used internally in GoogleTest tests or advanced test scenarios.

---

## Summary

Assertions in GoogleTest are your primary tools to verify program correctness in test cases. Choosing the right assertion type—fatal or non-fatal—and augmenting failures with meaningful messages allows you to write robust and maintainable tests that provide clear feedback. This page has guided you through understanding assertion semantics and practical usage, empowering you to leverage testing outcomes effectively.

For a comprehensive list of assertion macros and detailed examples, refer to the [Assertions Reference](../api-reference/core-test-api/assertions-reference).

---

## See Also

- [Using Assertions and Matchers](../../guides/core-workflows/using-assertions-and-matchers) — Practical guide on using assertions with matchers.
- [GoogleMock Mocking API](../../api-reference/mocking-api/mock-class-creation) — For mocking interactions that generate assertions.
- [Writing Your First Test](../../guides/getting-started/writing-your-first-test) — Overview of test writing including assertions.
- [Testing Lifecycle](../core-architecture/test-lifecycle) — Understand when assertions are evaluated in test flow.

---

<Callout type="tip">
Use `ASSERT_` to halt further execution if the test cannot proceed meaningfully after failure; otherwise, favor `EXPECT_` to identify multiple independent issues.
</Callout>

<Callout type="note">
Streaming detailed failure messages with `<<` greatly improves clarity and speeds debugging.
</Callout>


