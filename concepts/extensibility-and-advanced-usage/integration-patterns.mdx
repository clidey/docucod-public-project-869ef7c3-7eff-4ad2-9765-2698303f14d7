---
title: "Integration Patterns"
description: "Explore best practices and architectural considerations for integrating GoogleTest and GoogleMock into diverse build systems and CI environments. Understand where and how the test runner fits and how to orchestrate test execution across platforms."
---

# Integration Patterns

Explore best practices and architectural considerations for integrating GoogleTest and GoogleMock into diverse build systems and CI environments. Understand where and how the test runner fits and how to orchestrate test execution across platforms.

---

## Introduction

Integrating GoogleTest and GoogleMock into your build system and continuous integration (CI) pipeline is essential to fully harness their power in automating unit testing and behavior verification. This guide helps you understand the integration landscape, focusing on how the test runner fits within your environments and the best ways to organize test execution for maximum reliability and maintainability.

Whether you use CMake, Bazel, or other build systems, or work across Linux, Windows, or macOS, adopting these integration patterns will standardize your testing workflows.

---

## The Role of the Test Runner

At the heart of integration lies the test runner â€“ the executable or command that invokes GoogleTest's test harness (`RUN_ALL_TESTS()`) and reports results. Understanding its function is crucial:

- **Discovery:** It searches for all defined tests and test suites within the compiled binaries.
- **Execution:** Runs each test independently, respecting filters, randomized orders, sharding (test splitting), and repeat counts.
- **Reporting:** Outputs results to standard output, XML/JSON reports, or streams results for external consumption.

The test runner is typically a binary produced by your build process that includes your test code linked with the GoogleTest/GoogleMock frameworks.

### User Interaction with the Test Runner

Users commonly interact with the test runner via command-line arguments and environment variables.

- `--gtest_filter`: Selectively runs tests matching specified patterns.
- `--gtest_repeat`: Runs tests multiple times to catch flaky behavior.
- `--gtest_shuffle`: Randomizes test execution order.
- `--gtest_output`: Generates XML or JSON reports consumable by CI systems.

Additionally, the runner respects CI environment variables for sharding and parallel execution.

---

## Integration with Build Systems

GoogleTest and GoogleMock are designed for seamless incorporation into common C++ build systems. The following outlines best practices for two popular build environments.

### CMake Integration

1. **Fetch/Git Submodule:** Obtain GoogleTest sources as part of your project or through add_subdirectory.

2. **Target Linking:** Use the `target_link_libraries` directive to link your test targets against `gtest` and `gmock_main` as needed.

3. **Test Registration:** Larger projects use CMake's `add_test()` command to register the test binary for CTest and CI integration.

4. **Test Execution:** CI systems call the test binary (or invoke through CTest), passing arguments like `--gtest_output=xml:report.xml` for automated results.

5. **Advanced Features:** Configure test sharding via environment variables or flags to scale test execution across machines.

#### Sample CMake Snippet
```cmake
# Assuming googletest is downloaded in 'external/googletest'
add_subdirectory(external/googletest)

add_executable(my_tests my_tests.cc)
target_link_libraries(my_tests PRIVATE gtest gmock_main)

enable_testing()
add_test(NAME MyUnitTests COMMAND my_tests --gtest_output=xml:report.xml)
```

### Bazel Integration

1. **WORKSPACE Setup:** Declare GoogleTest and GoogleMock repositories.

2. **BUILD File:** Define `cc_test` targets with dependencies on the GoogleTest libraries.

3. **Test Execution:** Bazel runs tests automatically, handling test discovery, sharding, and retries, with integrated reporting.

4. **Flags & Attributes:** Use Bazel's test `args` to pass `--gtest_filter` or `--gtest_output` flags.

Example Bazel `cc_test`:
```starlark
cc_test(
    name = "my_tests",
    srcs = ["my_tests.cc"],
    deps = ["@com_google_googletest//:gtest_main"],
    args = ["--gtest_output=xml:report.xml"],
)
```

---

## Orchestrating Test Execution in CI Environments

Continuous integration environments require reliable and scalable test execution. GoogleTest supports important features to enable this:

### Test Filtering

Use `--gtest_filter` to limit which tests run in a given CI pipeline stage.

### Test Sharding

Environment variables `GTEST_TOTAL_SHARDS` and `GTEST_SHARD_INDEX` enable running subsets of tests on parallel machines or containers, dividing tests to speed up execution without overlap.

### Repeating Tests

Flaky tests can be caught with `--gtest_repeat` to run tests multiple times, exposing intermittent failures.

### Output Formats

CI tools can consume test results in XML or JSON formats, produced via `--gtest_output=xml:path` or `--gtest_output=json:path`.

### Handling Disabled and Flaky Tests

Use the `DISABLED_` prefix to temporarily exclude tests, and ensure CI includes `--gtest_also_run_disabled_tests` when validating those test fixes.

### Integration Example: GitHub Actions

```yaml
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Build
        run: cmake --build build
      - name: Run Tests
        run: |
          ./build/my_tests \
            --gtest_output=xml:report.xml \
            --gtest_shuffle \
            --gtest_repeat=3
      - name: Upload Test Report
        uses: actions/upload-artifact@v3
        with:
          name: test-report
          path: report.xml
```

---

## Advanced Integration Considerations

### Custom Test Event Listeners

GoogleTest supports adding event listeners for custom reporting or integration:

- You can append custom listeners using `UnitTest::GetInstance()->listeners().Append()`.
- Listeners receive callbacks on test start, end, failure, etc.

This allows integration with IDEs, dashboards, or specialized logging.

### Global and Suite Level Set-Up/Tear-Down

Define global test environments and per-suite setups to manage shared resources efficiently without rebuilding for every test:

```cpp
class GlobalEnv : public ::testing::Environment {
 public:
  void SetUp() override {
    // Initialize shared resources.
  }
  void TearDown() override {
    // Cleanup shared resources.
  }
};

::testing::AddGlobalTestEnvironment(new GlobalEnv);
```

### Mixing Test Fixtures and Test Suites

Ensure all tests within a suite use the same fixture class to avoid runtime errors.

### Handling Test Discovery in Large Repositories

- Use explicit test registration where automatic discovery is not feasible.
- Use parameterized or typed tests to reduce redundancy and improve scalability.

---

## Troubleshooting Integration

Many integration challenges arise from build mismatches, missing dependencies, or incorrect flag usage.

### Common Issues

- Tests not discovered/run: Check linking and test registration.
- Test output confusing: Ensure environment variables and flags control verbosity.
- Disabled tests not running: Use `--gtest_also_run_disabled_tests`.
- XML/JSON output missing/invalid: Confirm files are writable and flags correctly set.

Refer to the [Troubleshooting Guide](guides/integration-real-world/troubleshooting-common-issues) for deeper diagnostics.

---

## Summary

Integrating GoogleTest and GoogleMock seamlessly into your build and CI systems empowers you to automate testing effectively. Key integration points include setting up the test runner, configuring build dependencies, orchestrating parallel and repeated test execution, and managing test reporting and filters. Leveraging global setup/teardown, custom event listeners, and adhering to test fixture conventions helps maintain scalable and maintainable test infrastructures.

---

## References and Related Documentation

- [CMake Quickstart Guide](https://google.github.io/googletest/quickstart-cmake.html)
- [Bazel Integration Overview](https://docs.bazel.build)
- GoogleTest Primer and [Advanced Topics](overview/product-introduction/introduction-value,docs/advanced.md)
- [Cross-Platform CI Integration Guide](guides/integration-real-world/cross-platform-ci)
- [Writing Effective Tests: Parameterized and Typed Tests](guides/writing-effective-tests/test-parameterization)
- [Troubleshooting Common Issues](guides/integration-real-world/troubleshooting-common-issues)
- [Event Listener API](docs/advanced.md#extending-googletest-by-handling-test-events)

---

## Practical Tips

- Always return `RUN_ALL_TESTS()`'s result from your `main()` to correctly propagate test success/failure.
- Use the `--gtest_filter` and `--gtest_repeat` flags to control test scope and iteration in CI.
- Use test sharding environment variables to split test workloads across parallel machines.
- Register global test environments early to share expensive resources.
- Use custom event listeners for advanced test reporting needs.

---

For further integration patterns and examples, consult the [Integration & Build System Support](overview/feature-overview-integration/integration-points) and [Cross-Platform CI](guides/integration-real-world/cross-platform-ci) documentation.