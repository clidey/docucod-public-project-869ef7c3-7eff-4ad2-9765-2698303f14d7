---
title: "Test Results & Reporting"
description: "Details the structures and methods for capturing assertion outcomes, reporting test part results, and interpreting test outputs. Includes explanation of both fatal and non-fatal failures, and how test progress is displayed."
---

# Test Results & Reporting

GoogleTest provides a comprehensive framework for capturing, reporting, and interpreting the outcomes of tests. Understanding how test results are structured, how failures are classified, and how the framework communicates progress and status is essential to effectively leveraging GoogleTest to validate and improve your code.

---

## Overview of Test Result Structures

GoogleTest structures test outcomes across different abstraction levels: individual assertions (test parts), tests, test suites, and the entire test program. This layered reporting ensures that users gain clear visibility into which tests failed, what failed exactly, and the nature of those failures.

### Key Structures

- **TestPartResult**: Captures the outcome of a single test assertion or related event, including success, fatal failure, non-fatal failure, or skips.
- **TestResult**: Aggregates multiple `TestPartResult` objects for a single test. Contains timing and property metadata.
- **TestInfo**: Holds metadata and the `TestResult` for an individual test, such as its name, source location, and parameters.
- **TestSuite**: A logical grouping of tests with aggregated counts of successes, failures, skips, and timing.
- **UnitTest**: The global singleton exposing aggregate information about the entire test run.

These objects are accessible through the GoogleTest reflection API, letting you programmatically query test statuses and details.

## Capturing Assertion Outcomes

The fundamental building block of test reporting is the _assertion_. Assertions return one of the following outcomes on evaluation:

- **Success:** The condition passed.
- **Non-fatal Failure:** The check failed but the test continues.
- **Fatal Failure:** The check failed, aborting the current test function.
- **Skip:** The test or assertion was explicitly skipped.

Assertions are categorized primarily via macros:

- `ASSERT_*` macros cause fatal failures — they abort the current function immediately.
- `EXPECT_*` macros cause non-fatal failures — they report the error but continue execution.

For example, `ASSERT_EQ(x, y)` will stop test execution in the current function if `x != y`, while `EXPECT_EQ(x, y)` will report the failure but proceed.

### TestPartResult Details

Each assertion is recorded as a `TestPartResult` object containing:

- **Type:** Success, Fatal Failure, Nonfatal Failure, or Skip.
- **File name and line number:** Location of the assertion.
- **Summary and full message:** Failure summary and detailed diagnostic information.

You can access this data in custom test event listeners or via introspection.

## Reporting Test Progress and Results

GoogleTest communicates tests' progress and outcomes through its default console printer, XML/JSON report generators, and an extensible event listener API.

### Console Output

GoogleTest displays a concise progress log, marking each test as it runs:

```none
[ RUN      ] TestSuiteName.TestName
[       OK ] TestSuiteName.TestName (time ms)
[  FAILED  ] TestSuiteName.FailingTestName (time ms)
```

At the end, you get a summary indicating tests run, passed, and failed.

### XML and JSON Reports

For continuous integration systems or post-processing, GoogleTest can emit:

- **XML Reports:** Following a modified JUnit-compatible schema, detailing testsuites, testcases, and failures.
- **JSON Reports:** Conforming to a documented schema facilitating machine parsing.

To enable reports, pass the flag `--gtest_output=xml:<filename>` or `--gtest_output=json:<filename>`.

### Event Listeners

You can extend or replace GoogleTest output by creating custom listeners inheriting from `TestEventListener` or `EmptyTestEventListener`. These listeners receive callbacks for major events such as:

- Test program start/end
- Test suite start/end
- Individual test start/end
- Assertion results

This lets you implement GUIs, dashboards, or alternative reporting schemes.

## Differentiating Fatal and Non-Fatal Failures

GoogleTest distinctly treats failures based on their severity:

- **Fatal Failures:** Generated by `ASSERT_*` macros, halt execution of the current test function immediately. Use when continuing after failure is nonsensical or unsafe.
- **Non-fatal Failures:** Generated by `EXPECT_*` macros, allow the test to continue running to detect multiple failures in one run.

### User Guidance on Choosing Failure Types

- Use **non-fatal** assertions (`EXPECT_*`) when you want to collect as many failures as possible per test.
- Use **fatal** assertions (`ASSERT_*`) when a failure prevents meaningful execution of subsequent code, e.g., checking for null pointers before dereferencing.

## Interpreting Test Outcome through the API

GoogleTest provides utility queries to interpret results programmatically:

- `Test::HasFailure()`, `Test::HasFatalFailure()`, `Test::HasNonfatalFailure()` indicate the presence and type of failures.
- `TestResult::Passed()`, `Failed()`, `Skipped()` provide an aggregated test outcome.
- At higher levels, `TestSuite::Passed()`, `Failed()`, and `UnitTest::Passed()`, `Failed()` give overall status.

You can access the current test or suite via `UnitTest::GetInstance()->current_test_info()` or `current_test_suite()`, enabling dynamic behavior based on test progress.

## Displaying Test Progress and Final Results

GoogleTest's default console output reports each test's lifecycle:

- Begins with `[ RUN      ]` message
- On assertion results, detailed failure messages are printed, with file/line and message
- Ends with either `[       OK ]` or `[  FAILED  ]`

The summary gives a count of tests run, passed, failed, disabled, and skipped.

<Tip>
For silent or verbose output customization, explore flags like `--gtest_brief=1` to suppress passing test output, or implement custom event listeners for full control.
</Tip>

## Best Practices

- Prefer **non-fatal** failures (`EXPECT_*`) in tests to maximize feedback from a single run.
- Use **fatal** failures (`ASSERT_*`) judiciously to avoid segfaults or undefined behavior.
- Take advantage of `RecordProperty()` to embed additional metadata into tests that appear in XML reports.
- Use the event listener API to create tailored reporting or integrate with external monitoring tools.

<Warning>
Ignoring the return value of `RUN_ALL_TESTS()` can cause your test program to always report success even if failures occurred. Always propagate its return value.
</Warning>

## Troubleshooting Common Reporting Issues

- **No output or incomplete output:** Verify that `InitGoogleTest()` is called before `RUN_ALL_TESTS()`.
- **Missing detailed failure messages:** Ensure your assertions include custom messages or use predicate assertions for clarity.
- **Tests failing silently:** Check if fatal failures are causing early test termination, hiding other failures.

## Practical Example: Custom Event Listener Showing Test Progress

Below is an excerpt from an example minimalist printer illustrating hooking into test events for alternative reporting:

```cpp
class TersePrinter : public testing::EmptyTestEventListener {
 private:
  void OnTestStart(const testing::TestInfo& test_info) override {
    printf("*** Test %s.%s starting.\n", test_info.test_suite_name(), test_info.name());
  }

  void OnTestPartResult(const testing::TestPartResult& result) override {
    printf("%s in %s:%d\n%s\n",
           result.failed() ? "*** Failure" : "Success",
           result.file_name(), result.line_number(), result.summary());
  }

  void OnTestEnd(const testing::TestInfo& test_info) override {
    printf("*** Test %s.%s ending.\n", test_info.test_suite_name(), test_info.name());
  }

  void OnTestProgramEnd(const testing::UnitTest& unit_test) override {
    printf("TEST %s\n", unit_test.Passed() ? "PASSED" : "FAILED");
  }
};
```

## Summary

Understanding and utilizing GoogleTest's test results and reporting framework empowers you to track the health of your code effectively. By distinguishing fatal from non-fatal failures, leveraging detailed result objects, and customizing output through event listeners, you gain the flexibility and insight needed for robust test validation.

---