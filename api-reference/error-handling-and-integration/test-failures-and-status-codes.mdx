---
title: "Test Failures & Status Codes"
description: "Clarifies the interpretation of assertion results, error messages, and the range of available status codes (fatal, non-fatal, skipped) surfaced by GoogleTest/GoogleMock. Aids users in troubleshooting and automating test workflows based on status outputs."
---

# Test Failures & Status Codes

This page clarifies the interpretation of assertion results, error messages, and the range of available status codes surfaced by GoogleTest and GoogleMock. It helps you troubleshoot test failures effectively and automate workflows based on standardized status outputs.

---

## Understanding Test Part Results

GoogleTest breaks down test outcomes into granular units called *Test Part Results*. Each test assertion or explicit call to `FAIL()`, `ADD_FAILURE()`, or `SUCCEED()` generates a `TestPartResult` object that reflects the immediate result of that particular test operation.

### TestPartResult Types

The result of each test part is classified into one of the following types:

| Enum Value           | Meaning                                 | Description                                             |
|---------------------|-----------------------------------------|---------------------------------------------------------|
| `kSuccess`          | Success                                 | The test part passed successfully. No failure occurred. |
| `kNonFatalFailure`  | Non-Fatal Failure                       | The test part failed but did **not** stop the test flow. The test continues to run subsequent assertions. |
| `kFatalFailure`     | Fatal Failure                          | The test part failure is fatal and triggers immediate abort of the current test function (e.g., due to `ASSERT_*` macros). |
| `kSkip`             | Skipped                                | The test part was skipped at runtime, e.g. due to `GTEST_SKIP()`. Such parts are neither failures nor successes. |

### Accessing TestPartResult Properties

The `TestPartResult` class exposes several properties to better understand and programmatically respond to the test part's outcome:

- `type()`: Returns the result type as above.
- `file_name()`: The source file in which the test assertion took place, or `nullptr` if unknown.
- `line_number()`: The line number in the source file, or `-1` if unknown.
- `summary()`: A concise summary of the failure message excluding stack traces.
- `message()`: The full failure message including details and potentially stack traces.
- Status queries:
  - `passed()`: True if the test part passed.
  - `failed()`: True if the test part failed (either fatal or non-fatal).
  - `nonfatally_failed()`: True if test part non-fatally failed.
  - `fatally_failed()`: True if it caused a fatal failure.
  - `skipped()`: True if the test part was skipped.

### Example: Accessing TestPartResult

```cpp
// Assuming 'result' is a TestPartResult object
if (result.failed()) {
  std::cout << "Test part failed at " << (result.file_name() ? result.file_name() : "unknown file")
            << ":" << result.line_number() << std::endl;
  std::cout << "Message: " << result.message() << std::endl;
}
```

---

## How Test Results Are Aggregated

GoogleTest aggregates test part results into higher-level objects:

- **TestResult**: Represents the cumulative result of a single test case execution. It aggregates all `TestPartResult` instances generated by that test and determines whether the test passed, failed, or was skipped.
- **TestInfo**: Holds metadata about the test (name, location, parameters) and the `TestResult`.
- **TestSuite**: Contains multiple `TestInfo`s (tests) along with any setup/teardown failures aggregated in `ad_hoc_test_result()`.
- **UnitTest**: The top-level singleton representing the entire test program, aggregating all test suites and their results.

This structured hierarchy lets GoogleTest report detailed status and enables hooks for event listeners to track failures.

### Queries on TestResult

The `TestResult` class supports querying:

- `Passed()` — True if test passed (no failures, and not skipped).
- `Failed()` — True if any failures occurred.
- `Skipped()` — True if the test was skipped.
- `HasFatalFailure()` — True if any failure was fatal.
- `HasNonfatalFailure()` — True if any failure was non-fatal.
- Access individual `TestPartResult`s via `GetTestPartResult(index)`.


### Example: Examining TestResult

```cpp
const TestResult* result = test_info.result();
if (result->Failed()) {
  int failure_count = result->total_part_count();
  for (int i = 0; i < failure_count; ++i) {
    const TestPartResult& part = result->GetTestPartResult(i);
    if (part.failed()) {
      std::cout << "Failure: " << part.message() << " at "
                << part.file_name() << ":" << part.line_number() << std::endl;
    }
  }
}
```

---

## Interpreting Status Codes for Test Outcomes

GoogleTest normalizes test results into status categories that you can rely on for error handling and automation.

| Status Category | Description                                        | How it Affects Test Execution                        |
|-----------------|--------------------------------------------------|-----------------------------------------------------|
| Fatal Failure   | A fatal assertion triggered, usually via `ASSERT_*` macros | Aborts the current test immediately. Remaining test code is skipped. |
| Non-Fatal Failure | Test assertions failed but allow continued execution | Test continues to run to completion allowing multiple assertions to provide richer diagnostics. |
| Skipped         | Test or assertion was skipped explicitly (`GTEST_SKIP()`) | Test is marked as skipped; neither success nor failure. |
| Success         | Test executed with no failures                     | Normal successful completion.                        |

### Practical Implications

- Use `ASSERT_*` macros when failure should block further test execution.
- Use `EXPECT_*` macros if you want to log failures but continue testing.
- Use `GTEST_SKIP()` to dynamically skip tests, e.g., due to missing environment prerequisites.

---

## Typical Workflow: Troubleshooting Failures Using Status Codes

When a test fails, understanding its status codes helps you root cause and automate handling.

1. **Failure Detected**: Your test run reports a failure.

2. **Check Failure Type**: Determine if failure is fatal or non-fatal.

   - Fatal failures abort the current test immediately; often require fix before continuing.
   - Non-fatal failures allow other assertions in the test to execute; useful for multiple related checks.

3. **Identify Location**: Use `file_name()` and `line_number()` to locate the failing assertion.

4. **Analyze Failure Message**: Inspect the `message()` and `summary()` for diagnostic details.

5. **Handle Skipped Tests**: Recognize skipped tests and understand why the test was skipped via messages logged.

6. **Automate Process**: Use status queries (`failed()`, `skipped()`, `fatally_failed()`) programmatically to integrate with CI pipelines, e.g., set build results or alert developers.

---

## Best Practices and Tips

- Prefer `EXPECT_*` macros to collect multiple failures in one run unless a fatal failure is necessary.
- Use `GTEST_SKIP()` in setup functions or early test code to gracefully handle missing dependencies or conditions.
- Examine the summary message before the stack trace for a concise failure cause.
- In event listeners and custom reporters, check for skipped and failure states explicitly.
- Avoid mixing `TEST()` and `TEST_F()` in the same suite which can cause undefined test behavior.

---

## Common Pitfalls

- Misinterpretation of non-fatal failures as fatal.
- Ignoring the return status of `RUN_ALL_TESTS()` which reflects aggregate test success/failure.
- Failing to handle skipped tests in automation pipelines.
- Attempting to continue test execution after a fatal failure caused by `ASSERT_*` macros.

---

## Summary of Key Status Queries

| Status Query Function        | Meaning                                          |
|-----------------------------|--------------------------------------------------|
| `passed()`                  | Test part or test passed successfully.           |
| `failed()`                  | Test part or test failed (fatal or non-fatal).   |
| `fatally_failed()`          | Test part caused a fatal failure (aborts test). |
| `nonfatally_failed()`       | Test part caused failure but test continues.     |
| `skipped()`                 | Test part or entire test skipped at runtime.     |
| `TestResult::Passed()`      | Entire test passed (no failures, not skipped).   |
| `TestResult::Failed()`      | Entire test failed (any failures).                |
| `TestResult::Skipped()`     | Entire test was skipped.                           |

---

## Printing and Reporting Failures

GoogleTest provides default listeners such as `PrettyUnitTestResultPrinter` and `BriefUnitTestResultPrinter` which output failure messages with contextual data, including file, line, and accompanying stack traces unless suppressed. You can customize reporting by adding listeners to `UnitTest::listeners()` or use XML/JSON output options for integration with external tools.

### Example failure message output:

```
/path/to/source.cc:42: Failure
Expected equality of these values:
  foo
    Which is: 5
  bar
    Which is: 6

Stack trace:
  ... Google Test internal frames ...
  0x... in FooTest.MethodBar
  0x... in main
```

---

## Troubleshooting Common Issues

### Failure without location information

Sometimes you may see failures with `file_name()` as `nullptr` and line as `-1`. This generally implies the failure was generated programmatically or from outside normal assertion macros.

### Unexpected test being skipped

Verify if dynamic conditions in setup/teardown triggered `GTEST_SKIP()`. Also inspect flags for filtering, disabling, or sharding which might cause some tests not to run.

### Assertion reporting inconsistent with expectations

Ensure that you understand and respect the fatal vs non-fatal assertion distinction, and that your test framework integration honors the status codes.


---

_For more detailed reference of individual classes and methods related to test failures and results, see the [Testing Reference page](docs/reference/testing.md#TestPartResult), particularly the `TestPartResult` and `TestResult` classes._

_For error handling best practices and automation workflows, consult the Integration and Configuration guide._

---